#!/usr/bin/env python3

import sys
sys.path.append("..")

import pickle
import numpy as np

from nngp import GP, DKL, VIDKL, BNN
from nngp.kernels import RBFKernel

import argparse


def piecewise1():

    x_start = 0
    x_stop = 3

    def f(x):
        return np.piecewise(
            x, [x < 1.7, x >= 1.7],
            [lambda x: x**4.5, lambda x: 0.5*x**2.5])

    return (x_start, x_stop), f


def piecewise2():

    x_start = 0
    x_stop = 10

    def f(x):
        return np.piecewise(
            x, [x < 5, x >= 5],
            [lambda x: np.sin(x), lambda x: np.sin(x) + 3])

    return (x_start, x_stop), f


def piecewise3():

    x_start = 0
    x_stop = 3

    def f(x):
        return np.piecewise(
            x, [x < 1, (x >= 1) & (x < 2), x >= 2],
            [lambda x: 1 * x**2.5,
             lambda x: 0.5 * x**1.5 - 2,
             lambda x: 1 * np.exp(0.5 * (x - 2)) + 1]
             )
    
    return (x_start, x_stop), f



def nonstationary1():

    x_start = 0
    x_stop = 5.5

    def f(x):
        transition = 1 / (1 + np.exp(-10 * (x - np.pi))) 
        smooth_part = np.sin(2 * x)
        non_smooth_part = 0.3 * np.sin(10 * x) + 0.3 * np.cos(20 * x) + 0.3 * np.sin(x)**2
        return (1 - transition) * smooth_part + transition * non_smooth_part

    return (x_start, x_stop), f


def nonstationary2():

    x_start = -7
    x_stop = 7

    def f(x):
        y_smooth = np.sin(0.7 * x) * (np.abs(x) >= 2)
        y_non_smooth = np.sin(10 * x) * np.exp(-np.abs(2 * x)) * (np.abs(x) < 2)
        return y_smooth + y_non_smooth

    return (x_start, x_stop), f
    

def nonstationary3():

    x_start = 0
    x_stop = 10

    def f(x):
        spike_params = [
            (1, 2, 0.1),   # (amplitude, position, width)
            (-0.5, 4, 0.15),
            (1.5, 6, 0.1),
            (-1, 8, 0.1),
            (0.75, 9, 0.1)
        ]
        # Generate the smooth base curve
        y = np.sin(x)
        # Add each spike to the base curve
        for a, b, c in spike_params:
            y += a * np.exp(-((x - b)**2) / (2 * c**2))
        return y
    
    return (x_start, x_stop), f

    
def measure(fn, X, noise=0.05):
    """
    Generates noisy data using a specified synthetic function at given input point(s).

    Parameters:
    - fn: callable, the synthetic function to use for generating data.
    - X: np.ndarray, the input values at which to evaluate the function.
    - noise: float, the standard deviation of the Gaussian noise to add (default 0.05).

    Returns:
    - y_noisy: np.ndarray, the noisy output values generated by the function.
    """
    # Generate the output values from the function
    y = fn(X)

    # Add Gaussian noise
    y_noisy = y + np.random.normal(0, noise, X.shape)

    return y_noisy.squeeze()


def fit_predict(model_type, X, y, X_new, X_full=None, kernel=RBFKernel, latent_dim=2, **kwargs):
    """
    Initializes and trains a model based on the model_type argument.

    Parameters:
    - model_type: str, the type of the model ('GP', 'DKL', 'BNN', or 'VIDKL')
    - X: array-like, training input data
    - y: array-like, training target data
    - X_new: array-like, new/test inputs
    - latent_dim: int, the latent dimensionality for DKL and VIDKL
    - kernel: base kernel for GP, DKL, and viDKL. Defaults to RBFKernel.
    - **kwargs: additional arguments passed if necessary
    """

    input_dim = X.shape[-1]

    # Initialize the model based on the type
    if model_type == 'GP':
        model = GP(input_dim, kernel, **kwargs)
    elif model_type == 'DKL':
        model = DKL(input_dim, latent_dim, kernel, **kwargs)
    elif model_type == 'VIDKL':
        model = VIDKL(input_dim, latent_dim, kernel, **kwargs)
    elif model_type == 'BNN':
        model = BNN(input_dim, 1, **kwargs)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Train the model
    if model_type == 'VIDKL':
        model.fit(X, y, num_steps=5000, step_size=5e-3, **kwargs)
    else:
        model.fit(X, y, num_warmup=1000, num_samples=1000, **kwargs)

    posterior_mean, posterior_var = model.predict(X_new)

    if X_full is not None:
        posterior_mean_full, posterior_var_full = model.predict(X_full)
    else:
        posterior_mean_full = posterior_mean
        posterior_var_full = posterior_var

    return (posterior_mean, posterior_var), (posterior_mean_full, posterior_var_full)


def active_learning_loop(fn, model_type, X_initial, y_initial, X_candidate,
                         latent_dim=2, noise=0.05, n_steps=40, **kwargs):
    """
    Active learning routine
    """
    X_train = np.copy(X_initial)
    y_train = np.copy(y_initial)
    X_test = np.copy(X_candidate)
    y_test = fn(X_test).squeeze()

    history = {"posterior_means": [], "posterior_vars": [],
               "X_selected": [], "y_selected": [],
               "X_candidates": [], "mse": []}

    for step in range(n_steps):

        # Fit the model with the current training data and predict over the candidate points
        (posterior_mean, posterior_var), (posterior_mean_full, posterior_var_full) = fit_predict(
            model_type, X_train, y_train, X_candidate, X_test, latent_dim=latent_dim, **kwargs)
        
        # Select the next point to measure based on the highest variance
        max_var_idx = np.argmax(posterior_var.squeeze())
        X_next = X_candidate[max_var_idx].reshape(1, -1)
        y_next = measure(fn, X_next, noise=noise)
        
        # Update training set with the newly selected point
        X_train = np.append(X_train, X_next, axis=0)
        y_train = np.append(y_train, y_next)
        
        # Calculate MSE between the model's predictions and the true values over the full parameter space
        mse = np.mean((posterior_mean_full.squeeze() - y_test)**2)

        history["posterior_means"].append(posterior_mean_full)
        history["posterior_vars"].append(posterior_var_full)
        history["X_selected"].append(X_next)
        history["y_selected"].append(y_next)
        history["X_candidates"].append(X_candidate)
        history["mse"].append(mse)

        X_candidate = np.delete(X_candidate, max_var_idx, axis=0)

    return history


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run active learning routine for specified synthetic function and model type.')
    parser.add_argument('function_type', type=str, choices=['piecewise1', 'piecewise2', 'piecewise3', 'nonstationary1', 'nonstationary2', 'nonstationary3'],
                        help='Type of synthetic function to use.')
    parser.add_argument('model_type', type=str, choices=['GP', 'DKL', 'BNN', 'VIDKL'],
                        help='Type of model to use for predictions.')
    parser.add_argument('--seed', type=int, default=0, help='Optional seed for reproducibility.')
    args = parser.parse_args()

    np.random.seed(args.seed)

    function_mapping = {
        'piecewise1': piecewise1,
        'piecewise2': piecewise2,
        'piecewise3': piecewise3,
        'nonstationary1': nonstationary1,
        'nonstationary2': nonstationary2,
        'nonstationary3': nonstationary3,
    }

    (x_start, x_stop), fn = function_mapping[args.function_type]()
    X_initial = np.linspace(x_start, x_stop, 4).reshape(-1, 1)
    y_initial = measure(fn, X_initial)
    X_candidate = np.linspace(x_start, x_stop, 200).reshape(-1, 1)

    noise_level = 0.1 if args.function_type.startswith('piecewise') else 0.05

    history = active_learning_loop(
        fn, args.model_type, X_initial, y_initial, X_candidate,
        n_steps=40, noise=noise_level)

    print("Active learning completed.")

    save_data = {
        "X_initial": X_initial,
        "y_initial": y_initial,
        "history": history
    }

    # Save data to disc
    filename = f"AL_{args.function_type}_{args.model_type}.pkl"

    with open(filename, 'wb') as file:
        pickle.dump(save_data, file)
    print(f"Data saved to {filename}.")
